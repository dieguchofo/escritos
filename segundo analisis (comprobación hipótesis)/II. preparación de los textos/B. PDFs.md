En total, conseguí 492 PDFs. Si alguien busca en TESIUNAM todos los trabajos de titulación de la carrera de letras inglesas, verá que hay muchas más. Hay dos razones por las que trabajé con 492 [CORREGIR]. 

# Por qué conseguí los textos que conseguí
La primera razón es que trabajé con los trabajos de titulación publicados del 2006 al 2023. Descargué los PDFs en el 2024, por lo que los trabajos más recientes de mi investigación son del 2023 (no quiero tener datos de un año que sigue en curso porque serían incompletos). Que los más recientes sean del 2023 tiene sentido, pero ¿por qué empezar con los del 2006 y no con anteriores? Después de todo, el primer trabajo de la licenciatura fue publicado en 1962. La respuesta es que, antes del 2006, la gran mayoría los trabajos de titulación que alberga TESIUNAM son documentos escaneados, mientras que del 2006 en adelante todos son documentos digitales de origen. 

Procesar PDFs producidos directamente desde un procesador de textos es muchísimo más sencillo que procesar PDFs producidos por un escáner, pues los caracateres vienen ya codificados como caracteres, mientras que en los documentos escaneados vienen codificados como pixeles. Para usar PDFs producidos con un escáner se tiene que llevar a cabo uno de varios procesos. El más sencillo y que da resultados de mejor calidad es transcribir manualmente los textos. Este método, claramente, consume mucho tiempo y fuerza de trabajo, y por lo tanto nunca fue una opción que consideré (éste es el método empleado por el proyecto Gutenberg, por ejemplo). Otro método es el llamado reconocimiento óptico de caracteres (OCR, por sus siglas en inglés). El OCR utiliza software que reconoce patrones en los pixeles de una foto y los interpreta como caracteres. Éste método, a pesar de ser mucho más rápido que el primer método, no da resultados lo suficientemente fidedignos como para confiarles un estudio de esta índole (muchos de los libros en la base de datos de Google Books están codificados con este método, y si alguien ha intentado copiar y pegar texto de esos documentos, sabrá que no da los resultados más confiables). Ninguno de estos dos métodos se acopló a mi metodología, por lo que decidí olvidarme de los PDFs escaneados. Aún así, cuando se realiza la búsqueda en TESIUNAM acotando los resultados a este rango de años, la página nos dice que hay  498 trabajos, mientras que yo digo que conseguí 492. 

La segunda razón por la que trabajé con 492 documentos es que hay ciertos trabajos cuyas fichas aparecen en la página de TESIUNAM, pero que no están disponibles. Es posible que lx autorx no haya querido publicar su trabajo en TESIUNAM, como es el caso del trabajo "Metanarrativas y el emtramado [sic] en Enduring love de Ian Mcewan", o que, por alguna razón que TESIUNAM no hace pública, simple y sencillamente no hay botón para descargar el documento, como en el trabajo "The song and the water: el mar, la narración y la vida en The waves, de Virginia Woolf".

# El scraper
Conseguir los PDFs fue probablemente la etapa de la investigación que más trabajo me costó. No voy a entrar en detalle porque basta con decir que TESIUNAM está hecha de manera tal que los botones de las páginas de resultados que dicen "texto completo" no llevan directamente al texto completo, sino hace una serie de redirecciones que mi código tiene que seguir —un poquito como los procesos burocráticos en los que hay que ir a una oficina para que te digan que vayas a otra para que te digan que vayas a otra. No es nada muy complejo, solamente fue tedioso. Si alguien quiere ver los detalles del código, está en https://github.com/diego-g-fonte/crawler.