Antes que nada voy a resumir muy rápidamente todo lo que aconteció durante la investigación.

Descargué los PDFs de TESIUNAM, la base de datos de la UNAM que alberga y hace disponibles los trabajos de titulacicón de toda la universidad. Para hacerlo tuve que escribir unos scripts, escritos en Python (https://github.com/diego-g-fonte/crawler[^s.I.1]), que descargan todos los PDFs de todas las páginas de resultados de cualquier búsqueda en TESIUNAM, además de un documento JSON con los metadatos de dichos resultados (nombre de lx autorx, título, año de publicación y número de identificación del PDF asignado por TESIUNAM).

Para cuantificar el estilo usé una estrategia utilizada en los estudios de atribución autoral que está basada en las frecuencias relativas de las palabras. Frecuencia relativa sólo quiere decir que del 100% de las palabras de un texto, X% es la palabra "de", Y% es la palabra "la", etc. Por ejemplo, en la tesina titulada "El género policíaco y el gótico dentro de The virgin suicides de Jeffrey Eugenides", la palabra "de" conforma el 4.16% de las palabras, "la" el 3.29, etcétera. Generé, con el paquete "stylo", una tabla con las 5,000 palabras más comunes en todos los trabajos de titulación y su frecuencia relativa en cada uno. En estilometría, las frecuencias relativas de las palabras más comunes son lo que conforma el estilo de un texto. Ya teniendo la tabla con las frecuencias relativas, usé una función ("dist") que crea un espacio multidimensional (en este caso de 5,000 dimensiones) y coloca un punto por cada trabajo de titulación en ese espacio. Es más fácil pensar en un espacio de dos dimensiones en el que el eje X es la frecuencia relativa de la palabra "de", y el eje Y la de la palabra "la". Dentro de este espacio, el trabajo "El género policíaco ..." se convertiría en un punto con las coordenadas (4.16, 3.29). Otro trabajo de titulación, por ejemplo "Propuesta de lecciones-muestra para la enseñanza de la comprension de lectura en ingles en el marco de la ENA", tendría las coordenadas (7.11, 4.01). Podemos trazar una línea entre esos dos puntos y medir qué tan cerca o lejos están (en otras palabras, qué tan similares o diferentes son). La computadora puede hacer ese mismo proceso en 5,000 dimensiones y calcular las distancias entre todos los puntos. La primera gráfica que te envío ("estilo.png") visualiza el promedio de las distancias entre los trabajos de titulación publicados en un año, de los años 2006 al 2023. En palabras menos técnicas, si el punto es alto, los trabajos de ese año son diferentes, y si el punto es bajo, son similares.

Para cuantificar los temas utilicé una técnica llamada topic modeling. Usé un programa (MALLET, cuyas especificidades también me superan) que, al procesar los textos, produce una tabla con una cantidad X de temas (yo, arbitrariamente, elegí 50) y la cantidad de ese tema que tiene cada texto. Lo podemos pensar como que cada texto tiene un 100% de "tema", del cual 50% puede ser Jane Austen, 25% narratología, etc. Con esa tabla usé el mismo método de las muchas dimensiones y el resultado es la gráfica "temas.png". Es decir, si en un año, el punto es alto, los temas de los trabajos fueron diferentes (según MALLET), y si el punto es bajo, fueron similares.


[^s.I.1]: Lo llamé "crawler" ingenuamente; es un scraper. La diferencia en realidad no importa para este trabajo, pero quiero que, si por alguna razón alguien que sepa del tema lee este trabajo, sepa que estoy consciente de mi error.